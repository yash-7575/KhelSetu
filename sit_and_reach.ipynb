{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d77b95e4",
   "metadata": {},
   "source": [
    "# Sit and Reach Fitness Test Analysis System\n",
    "\n",
    "This notebook provides a comprehensive system for analyzing \"Sit and Reach\" fitness test videos using MediaPipe Pose estimation.\n",
    "\n",
    "## Features:\n",
    "- **Pose Detection**: Uses MediaPipe Pose for robust human keypoint detection\n",
    "- **Angle Measurement**: Accurately measures waist/torso angle throughout the test\n",
    "- **Cycle Counting**: Detects and counts proper reach attempts\n",
    "- **Threshold Validation**: Validates reaches against configurable thresholds\n",
    "- **Visualization**: Annotates video frames with landmarks and measurements\n",
    "- **Comprehensive Analysis**: Provides detailed statistics and insights\n",
    "\n",
    "## Requirements:\n",
    "- Python 3.7+\n",
    "- MediaPipe\n",
    "- OpenCV\n",
    "- NumPy\n",
    "- Matplotlib\n",
    "- Pandas (for data analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bebd2fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "OpenCV version: 4.11.0\n",
      "MediaPipe version: 0.10.9\n",
      "NumPy version: 1.24.3\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import math\n",
    "from collections import deque\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up matplotlib for better plots\n",
    "plt.style.use('default')\n",
    "%matplotlib inline\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "print(f\"MediaPipe version: {mp.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ac3830",
   "metadata": {},
   "source": [
    "## Configuration and Constants\n",
    "\n",
    "Define key parameters for the analysis including pose detection settings, angle thresholds, and visualization options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d2dad97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "class SitReachConfig:\n",
    "    \"\"\"Configuration class for Sit and Reach analysis\"\"\"\n",
    "    \n",
    "    # MediaPipe Pose settings\n",
    "    POSE_CONFIDENCE = 0.7\n",
    "    POSE_TRACKING_CONFIDENCE = 0.5\n",
    "    \n",
    "    # Angle thresholds (in degrees)\n",
    "    MIN_REACH_ANGLE = 45.0  # Minimum angle to consider as a reach attempt\n",
    "    PROPER_REACH_THRESHOLD = 60.0  # Angle threshold for \"proper\" reach\n",
    "    MAX_REASONABLE_ANGLE = 120.0  # Maximum reasonable angle (for filtering outliers)\n",
    "    \n",
    "    # Cycle detection parameters\n",
    "    SMOOTHING_WINDOW = 5  # Frames to smooth angle measurements\n",
    "    MIN_CYCLE_DURATION = 15  # Minimum frames between reach cycles\n",
    "    ANGLE_CHANGE_THRESHOLD = 10.0  # Minimum angle change to detect movement\n",
    "    \n",
    "    # Visualization settings\n",
    "    LANDMARK_COLOR = (0, 255, 0)  # Green\n",
    "    CONNECTION_COLOR = (255, 0, 0)  # Red\n",
    "    ANGLE_TEXT_COLOR = (255, 255, 255)  # White\n",
    "    PROPER_REACH_COLOR = (0, 255, 0)  # Green\n",
    "    IMPROPER_REACH_COLOR = (0, 0, 255)  # Red\n",
    "    \n",
    "    # Output settings\n",
    "    SAVE_ANNOTATED_VIDEO = True\n",
    "    SAVE_ANALYSIS_PLOTS = True\n",
    "    EXPORT_DATA_CSV = True\n",
    "\n",
    "config = SitReachConfig()\n",
    "print(\"‚úÖ Configuration loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a701a50",
   "metadata": {},
   "source": [
    "## Core Functions\n",
    "\n",
    "### 1. Pose Extraction and Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e31fd8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pose extractor initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "class PoseExtractor:\n",
    "    \"\"\"\n",
    "    Handles pose extraction and landmark processing using MediaPipe Pose\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, confidence=0.7, tracking_confidence=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the pose extractor\n",
    "        \n",
    "        Args:\n",
    "            confidence: Minimum confidence for pose detection\n",
    "            tracking_confidence: Minimum confidence for pose tracking\n",
    "        \"\"\"\n",
    "        self.pose = mp_pose.Pose(\n",
    "            static_image_mode=False,\n",
    "            model_complexity=2,  # Higher complexity for better accuracy\n",
    "            enable_segmentation=False,\n",
    "            min_detection_confidence=confidence,\n",
    "            min_tracking_confidence=tracking_confidence\n",
    "        )\n",
    "        \n",
    "    def extract_pose(self, frame: np.ndarray) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Extract pose landmarks from a single frame\n",
    "        \n",
    "        Args:\n",
    "            frame: Input video frame (BGR format)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing landmarks and metadata, or None if no pose detected\n",
    "        \"\"\"\n",
    "        # Convert BGR to RGB for MediaPipe\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process the frame\n",
    "        results = self.pose.process(rgb_frame)\n",
    "        \n",
    "        if results.pose_landmarks is None:\n",
    "            return None\n",
    "            \n",
    "        # Extract landmark coordinates\n",
    "        landmarks = {}\n",
    "        for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "            landmarks[idx] = {\n",
    "                'x': landmark.x,\n",
    "                'y': landmark.y,\n",
    "                'z': landmark.z,\n",
    "                'visibility': landmark.visibility\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'landmarks': landmarks,\n",
    "            'pose_landmarks': results.pose_landmarks,\n",
    "            'frame_shape': frame.shape\n",
    "        }\n",
    "    \n",
    "    def get_landmark_coords(self, pose_data: Dict, landmark_idx: int, frame_shape: Tuple) -> Optional[Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Convert normalized landmark coordinates to pixel coordinates\n",
    "        \n",
    "        Args:\n",
    "            pose_data: Pose data from extract_pose()\n",
    "            landmark_idx: MediaPipe landmark index\n",
    "            frame_shape: Shape of the frame (height, width, channels)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (x, y) pixel coordinates, or None if landmark not visible enough\n",
    "        \"\"\"\n",
    "        if pose_data is None or landmark_idx not in pose_data['landmarks']:\n",
    "            return None\n",
    "            \n",
    "        landmark = pose_data['landmarks'][landmark_idx]\n",
    "        \n",
    "        # Check visibility threshold\n",
    "        if landmark['visibility'] < 0.5:\n",
    "            return None\n",
    "            \n",
    "        # Convert to pixel coordinates\n",
    "        h, w = frame_shape[:2]\n",
    "        x = int(landmark['x'] * w)\n",
    "        y = int(landmark['y'] * h)\n",
    "        \n",
    "        return (x, y)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        self.pose.close()\n",
    "\n",
    "# Test the pose extractor\n",
    "pose_extractor = PoseExtractor(\n",
    "    confidence=config.POSE_CONFIDENCE,\n",
    "    tracking_confidence=config.POSE_TRACKING_CONFIDENCE\n",
    ")\n",
    "print(\"‚úÖ Pose extractor initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2020c066",
   "metadata": {},
   "source": [
    "### 2. Angle Calculation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e09a671c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Angle calculator initialized successfully!\n",
      "Test angle calculation: 90.0¬∞ (expected: 90.0¬∞)\n"
     ]
    }
   ],
   "source": [
    "class AngleCalculator:\n",
    "    \"\"\"\n",
    "    Handles angle calculations for the sit and reach test\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_angle(point1: Tuple[float, float], \n",
    "                       point2: Tuple[float, float], \n",
    "                       point3: Tuple[float, float]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate angle between three points (point2 is the vertex)\n",
    "        \n",
    "        Args:\n",
    "            point1: First point coordinates (x, y)\n",
    "            point2: Vertex point coordinates (x, y)\n",
    "            point3: Third point coordinates (x, y)\n",
    "            \n",
    "        Returns:\n",
    "            Angle in degrees\n",
    "        \"\"\"\n",
    "        # Create vectors\n",
    "        vector1 = np.array([point1[0] - point2[0], point1[1] - point2[1]])\n",
    "        vector2 = np.array([point3[0] - point2[0], point3[1] - point2[1]])\n",
    "        \n",
    "        # Calculate angle using dot product\n",
    "        dot_product = np.dot(vector1, vector2)\n",
    "        norms = np.linalg.norm(vector1) * np.linalg.norm(vector2)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if norms == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        # Calculate angle in radians then convert to degrees\n",
    "        cos_angle = np.clip(dot_product / norms, -1.0, 1.0)\n",
    "        angle_rad = np.arccos(cos_angle)\n",
    "        angle_deg = np.degrees(angle_rad)\n",
    "        \n",
    "        return angle_deg\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_waist_angle(pose_data: Dict, frame_shape: Tuple, \n",
    "                             pose_extractor: PoseExtractor) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Calculate the waist/torso angle for sit and reach analysis\n",
    "        \n",
    "        The angle is measured between:\n",
    "        - Hip to shoulder line (torso)\n",
    "        - Hip to knee line (thigh)\n",
    "        \n",
    "        Args:\n",
    "            pose_data: Pose landmarks data\n",
    "            frame_shape: Frame dimensions\n",
    "            pose_extractor: PoseExtractor instance for coordinate conversion\n",
    "            \n",
    "        Returns:\n",
    "            Waist angle in degrees, or None if landmarks not available\n",
    "        \"\"\"\n",
    "        if pose_data is None:\n",
    "            return None\n",
    "            \n",
    "        # MediaPipe landmark indices\n",
    "        # Using left side landmarks (can be adjusted based on video orientation)\n",
    "        LEFT_SHOULDER = 11\n",
    "        LEFT_HIP = 23\n",
    "        LEFT_KNEE = 25\n",
    "        \n",
    "        # Get landmark coordinates\n",
    "        shoulder = pose_extractor.get_landmark_coords(pose_data, LEFT_SHOULDER, frame_shape)\n",
    "        hip = pose_extractor.get_landmark_coords(pose_data, LEFT_HIP, frame_shape)\n",
    "        knee = pose_extractor.get_landmark_coords(pose_data, LEFT_KNEE, frame_shape)\n",
    "        \n",
    "        # Check if all landmarks are available\n",
    "        if not all([shoulder, hip, knee]):\n",
    "            # Try right side landmarks as backup\n",
    "            RIGHT_SHOULDER = 12\n",
    "            RIGHT_HIP = 24\n",
    "            RIGHT_KNEE = 26\n",
    "            \n",
    "            shoulder = pose_extractor.get_landmark_coords(pose_data, RIGHT_SHOULDER, frame_shape)\n",
    "            hip = pose_extractor.get_landmark_coords(pose_data, RIGHT_HIP, frame_shape)\n",
    "            knee = pose_extractor.get_landmark_coords(pose_data, RIGHT_KNEE, frame_shape)\n",
    "            \n",
    "            if not all([shoulder, hip, knee]):\n",
    "                return None\n",
    "        \n",
    "        # Calculate angle with hip as vertex\n",
    "        angle = AngleCalculator.calculate_angle(shoulder, hip, knee)\n",
    "        \n",
    "        # For sit and reach, we want the forward bending angle\n",
    "        # Adjust angle calculation based on the expected range\n",
    "        if angle > 90:\n",
    "            angle = 180 - angle\n",
    "            \n",
    "        return angle\n",
    "    \n",
    "    @staticmethod\n",
    "    def smooth_angles(angles: List[float], window_size: int = 5) -> List[float]:\n",
    "        \"\"\"\n",
    "        Apply smoothing to angle measurements to reduce noise\n",
    "        \n",
    "        Args:\n",
    "            angles: List of angle measurements\n",
    "            window_size: Size of the smoothing window\n",
    "            \n",
    "        Returns:\n",
    "            Smoothed angle measurements\n",
    "        \"\"\"\n",
    "        if len(angles) < window_size:\n",
    "            return angles\n",
    "            \n",
    "        smoothed = []\n",
    "        for i in range(len(angles)):\n",
    "            start_idx = max(0, i - window_size // 2)\n",
    "            end_idx = min(len(angles), i + window_size // 2 + 1)\n",
    "            window_angles = [a for a in angles[start_idx:end_idx] if a is not None]\n",
    "            \n",
    "            if window_angles:\n",
    "                smoothed.append(np.mean(window_angles))\n",
    "            else:\n",
    "                smoothed.append(angles[i])\n",
    "                \n",
    "        return smoothed\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_valid_angle(angle: Optional[float]) -> bool:\n",
    "        \"\"\"\n",
    "        Check if an angle measurement is valid\n",
    "        \n",
    "        Args:\n",
    "            angle: Angle measurement to validate\n",
    "            \n",
    "        Returns:\n",
    "            True if angle is valid, False otherwise\n",
    "        \"\"\"\n",
    "        if angle is None:\n",
    "            return False\n",
    "        return 0 <= angle <= config.MAX_REASONABLE_ANGLE\n",
    "\n",
    "# Test angle calculator\n",
    "angle_calc = AngleCalculator()\n",
    "print(\"‚úÖ Angle calculator initialized successfully!\")\n",
    "\n",
    "# Test angle calculation with sample points\n",
    "test_angle = angle_calc.calculate_angle((0, 0), (1, 0), (1, 1))\n",
    "print(f\"Test angle calculation: {test_angle:.1f}¬∞ (expected: 90.0¬∞)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a866bfc4",
   "metadata": {},
   "source": [
    "### 3. Cycle Detection and Reach Counting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b5c944c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cycle detector initialized successfully!\n",
      "Test analysis - Total reaches: 1, Proper reaches: 1\n"
     ]
    }
   ],
   "source": [
    "class CycleDetector:\n",
    "    \"\"\"\n",
    "    Detects reach cycles and counts proper reaches in the sit and reach test\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_cycle_duration: int = 15, angle_change_threshold: float = 10.0):\n",
    "        \"\"\"\n",
    "        Initialize cycle detector\n",
    "        \n",
    "        Args:\n",
    "            min_cycle_duration: Minimum frames between reach cycles\n",
    "            angle_change_threshold: Minimum angle change to detect movement\n",
    "        \"\"\"\n",
    "        self.min_cycle_duration = min_cycle_duration\n",
    "        self.angle_change_threshold = angle_change_threshold\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset detector state\"\"\"\n",
    "        self.angle_history = deque(maxlen=100)  # Keep last 100 angle measurements\n",
    "        self.reach_cycles = []\n",
    "        self.current_cycle = None\n",
    "        self.last_cycle_end = -self.min_cycle_duration\n",
    "        \n",
    "    def detect_peaks_and_valleys(self, angles: List[float]) -> Tuple[List[int], List[int]]:\n",
    "        \"\"\"\n",
    "        Detect peaks (maximum reach) and valleys (minimum reach) in angle data\n",
    "        \n",
    "        Args:\n",
    "            angles: List of angle measurements\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (peak_indices, valley_indices)\n",
    "        \"\"\"\n",
    "        if len(angles) < 3:\n",
    "            return [], []\n",
    "            \n",
    "        peaks = []\n",
    "        valleys = []\n",
    "        \n",
    "        for i in range(1, len(angles) - 1):\n",
    "            if angles[i] is None:\n",
    "                continue\n",
    "                \n",
    "            prev_angle = angles[i-1] if angles[i-1] is not None else angles[i]\n",
    "            next_angle = angles[i+1] if angles[i+1] is not None else angles[i]\n",
    "            \n",
    "            # Peak detection (local maximum)\n",
    "            if angles[i] > prev_angle and angles[i] > next_angle:\n",
    "                if angles[i] >= config.MIN_REACH_ANGLE:\n",
    "                    peaks.append(i)\n",
    "            \n",
    "            # Valley detection (local minimum)\n",
    "            elif angles[i] < prev_angle and angles[i] < next_angle:\n",
    "                valleys.append(i)\n",
    "                \n",
    "        return peaks, valleys\n",
    "    \n",
    "    def process_frame_angle(self, angle: Optional[float], frame_idx: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a single frame's angle measurement for cycle detection\n",
    "        \n",
    "        Args:\n",
    "            angle: Current frame's waist angle\n",
    "            frame_idx: Current frame index\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with cycle detection results for this frame\n",
    "        \"\"\"\n",
    "        result = {\n",
    "            'frame_idx': frame_idx,\n",
    "            'angle': angle,\n",
    "            'new_reach_detected': False,\n",
    "            'reach_completed': False,\n",
    "            'is_proper_reach': False,\n",
    "            'cycle_info': None\n",
    "        }\n",
    "        \n",
    "        if angle is None or not AngleCalculator.is_valid_angle(angle):\n",
    "            return result\n",
    "            \n",
    "        self.angle_history.append((frame_idx, angle))\n",
    "        \n",
    "        # Need at least a few frames to detect cycles\n",
    "        if len(self.angle_history) < 5:\n",
    "            return result\n",
    "            \n",
    "        # Get recent angles for analysis\n",
    "        recent_angles = [a[1] for a in list(self.angle_history)[-10:]]\n",
    "        recent_frames = [a[0] for a in list(self.angle_history)[-10:]]\n",
    "        \n",
    "        # Detect if we're starting a new reach (angle increasing significantly)\n",
    "        if self.current_cycle is None:\n",
    "            if len(recent_angles) >= 3:\n",
    "                angle_trend = recent_angles[-1] - recent_angles[-3]\n",
    "                if (angle_trend > self.angle_change_threshold and \n",
    "                    frame_idx - self.last_cycle_end >= self.min_cycle_duration and\n",
    "                    angle >= config.MIN_REACH_ANGLE):\n",
    "                    \n",
    "                    # Start new cycle\n",
    "                    self.current_cycle = {\n",
    "                        'start_frame': frame_idx,\n",
    "                        'start_angle': angle,\n",
    "                        'max_angle': angle,\n",
    "                        'max_angle_frame': frame_idx,\n",
    "                        'angles': [(frame_idx, angle)],\n",
    "                        'is_ascending': True\n",
    "                    }\n",
    "                    result['new_reach_detected'] = True\n",
    "        \n",
    "        # Update current cycle if active\n",
    "        elif self.current_cycle is not None:\n",
    "            self.current_cycle['angles'].append((frame_idx, angle))\n",
    "            \n",
    "            # Update maximum angle\n",
    "            if angle > self.current_cycle['max_angle']:\n",
    "                self.current_cycle['max_angle'] = angle\n",
    "                self.current_cycle['max_angle_frame'] = frame_idx\n",
    "                self.current_cycle['is_ascending'] = True\n",
    "            \n",
    "            # Check if we're descending (reach completed)\n",
    "            elif (self.current_cycle['is_ascending'] and \n",
    "                  len(recent_angles) >= 3 and\n",
    "                  angle < self.current_cycle['max_angle'] - self.angle_change_threshold):\n",
    "                \n",
    "                # Complete the cycle\n",
    "                self.current_cycle['end_frame'] = frame_idx\n",
    "                self.current_cycle['end_angle'] = angle\n",
    "                self.current_cycle['duration'] = frame_idx - self.current_cycle['start_frame']\n",
    "                \n",
    "                # Validate the reach\n",
    "                is_proper = self.current_cycle['max_angle'] >= config.PROPER_REACH_THRESHOLD\n",
    "                self.current_cycle['is_proper_reach'] = is_proper\n",
    "                \n",
    "                # Add to completed cycles\n",
    "                self.reach_cycles.append(self.current_cycle.copy())\n",
    "                \n",
    "                result['reach_completed'] = True\n",
    "                result['is_proper_reach'] = is_proper\n",
    "                result['cycle_info'] = self.current_cycle.copy()\n",
    "                \n",
    "                # Reset for next cycle\n",
    "                self.last_cycle_end = frame_idx\n",
    "                self.current_cycle = None\n",
    "                \n",
    "        return result\n",
    "    \n",
    "    def analyze_complete_sequence(self, angles: List[float]) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze a complete sequence of angles to detect all cycles\n",
    "        \n",
    "        Args:\n",
    "            angles: Complete list of angle measurements\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with comprehensive cycle analysis\n",
    "        \"\"\"\n",
    "        self.reset()\n",
    "        \n",
    "        # Process each frame\n",
    "        frame_results = []\n",
    "        for frame_idx, angle in enumerate(angles):\n",
    "            result = self.process_frame_angle(angle, frame_idx)\n",
    "            frame_results.append(result)\n",
    "        \n",
    "        # Complete any ongoing cycle at the end\n",
    "        if self.current_cycle is not None:\n",
    "            self.current_cycle['end_frame'] = len(angles) - 1\n",
    "            self.current_cycle['end_angle'] = angles[-1] if angles[-1] is not None else 0\n",
    "            self.current_cycle['duration'] = self.current_cycle['end_frame'] - self.current_cycle['start_frame']\n",
    "            self.current_cycle['is_proper_reach'] = self.current_cycle['max_angle'] >= config.PROPER_REACH_THRESHOLD\n",
    "            self.reach_cycles.append(self.current_cycle)\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        total_reaches = len(self.reach_cycles)\n",
    "        proper_reaches = sum(1 for cycle in self.reach_cycles if cycle['is_proper_reach'])\n",
    "        \n",
    "        max_angles = [cycle['max_angle'] for cycle in self.reach_cycles if cycle['max_angle'] is not None]\n",
    "        overall_max_angle = max(max_angles) if max_angles else 0\n",
    "        average_max_angle = np.mean(max_angles) if max_angles else 0\n",
    "        \n",
    "        return {\n",
    "            'total_reaches': total_reaches,\n",
    "            'proper_reaches': proper_reaches,\n",
    "            'improper_reaches': total_reaches - proper_reaches,\n",
    "            'overall_max_angle': overall_max_angle,\n",
    "            'average_max_angle': average_max_angle,\n",
    "            'reach_cycles': self.reach_cycles,\n",
    "            'frame_results': frame_results,\n",
    "            'success_rate': (proper_reaches / total_reaches * 100) if total_reaches > 0 else 0\n",
    "        }\n",
    "\n",
    "# Test cycle detector\n",
    "cycle_detector = CycleDetector(\n",
    "    min_cycle_duration=config.MIN_CYCLE_DURATION,\n",
    "    angle_change_threshold=config.ANGLE_CHANGE_THRESHOLD\n",
    ")\n",
    "print(\"‚úÖ Cycle detector initialized successfully!\")\n",
    "\n",
    "# Test with sample angle data\n",
    "test_angles = [30, 35, 45, 60, 75, 70, 50, 35, 40, 55, 80, 65, 45, 30]\n",
    "test_analysis = cycle_detector.analyze_complete_sequence(test_angles)\n",
    "print(f\"Test analysis - Total reaches: {test_analysis['total_reaches']}, Proper reaches: {test_analysis['proper_reaches']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef1675",
   "metadata": {},
   "source": [
    "### 4. Visualization and Video Annotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5367a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Video annotator initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "class VideoAnnotator:\n",
    "    \"\"\"\n",
    "    Handles video annotation and visualization for pose analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: SitReachConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def draw_pose_landmarks(self, frame: np.ndarray, pose_data: Dict) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Draw pose landmarks on the frame\n",
    "        \n",
    "        Args:\n",
    "            frame: Input frame\n",
    "            pose_data: Pose landmarks data\n",
    "            \n",
    "        Returns:\n",
    "            Annotated frame\n",
    "        \"\"\"\n",
    "        if pose_data is None or 'pose_landmarks' not in pose_data:\n",
    "            return frame\n",
    "            \n",
    "        annotated_frame = frame.copy()\n",
    "        \n",
    "        # Draw pose landmarks\n",
    "        mp_drawing.draw_landmarks(\n",
    "            annotated_frame,\n",
    "            pose_data['pose_landmarks'],\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style()\n",
    "        )\n",
    "        \n",
    "        return annotated_frame\n",
    "    \n",
    "    def draw_angle_measurement(self, frame: np.ndarray, pose_data: Dict, \n",
    "                             angle: Optional[float], pose_extractor: PoseExtractor) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Draw angle measurement and key points on the frame\n",
    "        \n",
    "        Args:\n",
    "            frame: Input frame\n",
    "            pose_data: Pose landmarks data\n",
    "            angle: Calculated angle\n",
    "            pose_extractor: PoseExtractor instance\n",
    "            \n",
    "        Returns:\n",
    "            Annotated frame\n",
    "        \"\"\"\n",
    "        if pose_data is None or angle is None:\n",
    "            return frame\n",
    "            \n",
    "        annotated_frame = frame.copy()\n",
    "        frame_shape = frame.shape\n",
    "        \n",
    "        # Get key landmark positions\n",
    "        LEFT_SHOULDER = 11\n",
    "        LEFT_HIP = 23\n",
    "        LEFT_KNEE = 25\n",
    "        \n",
    "        shoulder = pose_extractor.get_landmark_coords(pose_data, LEFT_SHOULDER, frame_shape)\n",
    "        hip = pose_extractor.get_landmark_coords(pose_data, LEFT_HIP, frame_shape)\n",
    "        knee = pose_extractor.get_landmark_coords(pose_data, LEFT_KNEE, frame_shape)\n",
    "        \n",
    "        # If left side not available, try right side\n",
    "        if not all([shoulder, hip, knee]):\n",
    "            RIGHT_SHOULDER = 12\n",
    "            RIGHT_HIP = 24\n",
    "            RIGHT_KNEE = 26\n",
    "            \n",
    "            shoulder = pose_extractor.get_landmark_coords(pose_data, RIGHT_SHOULDER, frame_shape)\n",
    "            hip = pose_extractor.get_landmark_coords(pose_data, RIGHT_HIP, frame_shape)\n",
    "            knee = pose_extractor.get_landmark_coords(pose_data, RIGHT_KNEE, frame_shape)\n",
    "        \n",
    "        if all([shoulder, hip, knee]):\n",
    "            # Draw angle lines\n",
    "            cv2.line(annotated_frame, shoulder, hip, self.config.CONNECTION_COLOR, 3)\n",
    "            cv2.line(annotated_frame, hip, knee, self.config.CONNECTION_COLOR, 3)\n",
    "            \n",
    "            # Draw key points\n",
    "            cv2.circle(annotated_frame, shoulder, 8, self.config.LANDMARK_COLOR, -1)\n",
    "            cv2.circle(annotated_frame, hip, 10, (0, 255, 255), -1)  # Yellow for hip (vertex)\n",
    "            cv2.circle(annotated_frame, knee, 8, self.config.LANDMARK_COLOR, -1)\n",
    "            \n",
    "            # Draw angle text\n",
    "            angle_text = f\"Angle: {angle:.1f}¬∞\"\n",
    "            text_position = (hip[0] + 20, hip[1] - 20)\n",
    "            \n",
    "            # Choose color based on angle threshold\n",
    "            text_color = self.config.PROPER_REACH_COLOR if angle >= self.config.PROPER_REACH_THRESHOLD else self.config.IMPROPER_REACH_COLOR\n",
    "            \n",
    "            cv2.putText(annotated_frame, angle_text, text_position, \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, text_color, 2)\n",
    "        \n",
    "        return annotated_frame\n",
    "    \n",
    "    def draw_reach_status(self, frame: np.ndarray, cycle_result: Dict, \n",
    "                         total_reaches: int, proper_reaches: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Draw reach status and counters on the frame\n",
    "        \n",
    "        Args:\n",
    "            frame: Input frame\n",
    "            cycle_result: Current frame's cycle detection result\n",
    "            total_reaches: Total reach count\n",
    "            proper_reaches: Proper reach count\n",
    "            \n",
    "        Returns:\n",
    "            Annotated frame\n",
    "        \"\"\"\n",
    "        annotated_frame = frame.copy()\n",
    "        h, w = frame.shape[:2]\n",
    "        \n",
    "        # Status panel background\n",
    "        panel_height = 120\n",
    "        cv2.rectangle(annotated_frame, (10, 10), (400, panel_height), (0, 0, 0), -1)\n",
    "        cv2.rectangle(annotated_frame, (10, 10), (400, panel_height), (255, 255, 255), 2)\n",
    "        \n",
    "        # Status text\n",
    "        y_offset = 35\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.6\n",
    "        \n",
    "        # Total reaches\n",
    "        cv2.putText(annotated_frame, f\"Total Reaches: {total_reaches}\", \n",
    "                   (20, y_offset), font, font_scale, (255, 255, 255), 2)\n",
    "        \n",
    "        # Proper reaches\n",
    "        y_offset += 25\n",
    "        cv2.putText(annotated_frame, f\"Proper Reaches: {proper_reaches}\", \n",
    "                   (20, y_offset), font, font_scale, self.config.PROPER_REACH_COLOR, 2)\n",
    "        \n",
    "        # Success rate\n",
    "        y_offset += 25\n",
    "        success_rate = (proper_reaches / total_reaches * 100) if total_reaches > 0 else 0\n",
    "        cv2.putText(annotated_frame, f\"Success Rate: {success_rate:.1f}%\", \n",
    "                   (20, y_offset), font, font_scale, (255, 255, 0), 2)\n",
    "        \n",
    "        # Current status\n",
    "        y_offset += 25\n",
    "        if cycle_result['new_reach_detected']:\n",
    "            status_text = \"NEW REACH STARTED\"\n",
    "            status_color = (0, 255, 255)  # Cyan\n",
    "        elif cycle_result['reach_completed']:\n",
    "            if cycle_result['is_proper_reach']:\n",
    "                status_text = \"PROPER REACH COMPLETED!\"\n",
    "                status_color = self.config.PROPER_REACH_COLOR\n",
    "            else:\n",
    "                status_text = \"Reach completed (below threshold)\"\n",
    "                status_color = self.config.IMPROPER_REACH_COLOR\n",
    "        else:\n",
    "            status_text = \"Monitoring...\"\n",
    "            status_color = (255, 255, 255)\n",
    "        \n",
    "        cv2.putText(annotated_frame, status_text, (20, y_offset), \n",
    "                   font, font_scale, status_color, 2)\n",
    "        \n",
    "        return annotated_frame\n",
    "    \n",
    "    def create_angle_plot(self, angles: List[float], reach_cycles: List[Dict], \n",
    "                         fps: float = 30.0) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Create a plot showing angle measurements over time\n",
    "        \n",
    "        Args:\n",
    "            angles: List of angle measurements\n",
    "            reach_cycles: List of detected reach cycles\n",
    "            fps: Video frame rate for time conversion\n",
    "            \n",
    "        Returns:\n",
    "            Matplotlib figure\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        # Convert frame indices to time\n",
    "        time_points = [i / fps for i in range(len(angles))]\n",
    "        valid_angles = [a if a is not None else np.nan for a in angles]\n",
    "        \n",
    "        # Plot angle measurements\n",
    "        ax.plot(time_points, valid_angles, 'b-', linewidth=2, label='Waist Angle')\n",
    "        \n",
    "        # Mark reach cycles\n",
    "        for i, cycle in enumerate(reach_cycles):\n",
    "            start_time = cycle['start_frame'] / fps\n",
    "            end_time = cycle['end_frame'] / fps\n",
    "            max_time = cycle['max_angle_frame'] / fps\n",
    "            \n",
    "            color = 'green' if cycle['is_proper_reach'] else 'red'\n",
    "            alpha = 0.3\n",
    "            \n",
    "            # Highlight reach period\n",
    "            ax.axvspan(start_time, end_time, alpha=alpha, color=color)\n",
    "            \n",
    "            # Mark maximum angle\n",
    "            ax.plot(max_time, cycle['max_angle'], 'o', color=color, markersize=8)\n",
    "            ax.annotate(f\"{cycle['max_angle']:.1f}¬∞\", \n",
    "                       (max_time, cycle['max_angle']), \n",
    "                       xytext=(5, 5), textcoords='offset points',\n",
    "                       fontsize=9, color=color)\n",
    "        \n",
    "        # Add threshold lines\n",
    "        ax.axhline(y=self.config.PROPER_REACH_THRESHOLD, color='green', \n",
    "                  linestyle='--', alpha=0.7, label=f'Proper Reach Threshold ({self.config.PROPER_REACH_THRESHOLD}¬∞)')\n",
    "        ax.axhline(y=self.config.MIN_REACH_ANGLE, color='orange', \n",
    "                  linestyle='--', alpha=0.7, label=f'Min Reach Angle ({self.config.MIN_REACH_ANGLE}¬∞)')\n",
    "        \n",
    "        ax.set_xlabel('Time (seconds)')\n",
    "        ax.set_ylabel('Waist Angle (degrees)')\n",
    "        ax.set_title('Sit and Reach Analysis - Angle Measurements Over Time')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "# Initialize video annotator\n",
    "video_annotator = VideoAnnotator(config)\n",
    "print(\"‚úÖ Video annotator initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d6bfe5",
   "metadata": {},
   "source": [
    "### 5. Main Video Processing Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14214885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sit and Reach Analyzer initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "class SitReachAnalyzer:\n",
    "    \"\"\"\n",
    "    Main class that orchestrates the complete sit and reach analysis pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: SitReachConfig):\n",
    "        self.config = config\n",
    "        self.pose_extractor = PoseExtractor(\n",
    "            confidence=config.POSE_CONFIDENCE,\n",
    "            tracking_confidence=config.POSE_TRACKING_CONFIDENCE\n",
    "        )\n",
    "        self.angle_calculator = AngleCalculator()\n",
    "        self.cycle_detector = CycleDetector(\n",
    "            min_cycle_duration=config.MIN_CYCLE_DURATION,\n",
    "            angle_change_threshold=config.ANGLE_CHANGE_THRESHOLD\n",
    "        )\n",
    "        self.video_annotator = VideoAnnotator(config)\n",
    "        \n",
    "    def analyze_video(self, video_path: str, output_dir: str = \"output\") -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze a complete video file for sit and reach performance\n",
    "        \n",
    "        Args:\n",
    "            video_path: Path to the input video file\n",
    "            output_dir: Directory to save output files\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing comprehensive analysis results\n",
    "        \"\"\"\n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Open video file\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video file: {video_path}\")\n",
    "        \n",
    "        # Get video properties\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        \n",
    "        print(f\"üìπ Processing video: {video_path}\")\n",
    "        print(f\"   üìä Properties: {width}x{height}, {fps:.1f} FPS, {frame_count} frames\")\n",
    "        print(f\"   ‚è±Ô∏è Duration: {frame_count/fps:.1f} seconds\")\n",
    "        \n",
    "        # Prepare output video writer if enabled\n",
    "        output_video_path = None\n",
    "        video_writer = None\n",
    "        if self.config.SAVE_ANNOTATED_VIDEO:\n",
    "            output_video_path = os.path.join(output_dir, f\"annotated_{os.path.basename(video_path)}\")\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        # Initialize tracking variables\n",
    "        all_angles = []\n",
    "        all_pose_data = []\n",
    "        frame_results = []\n",
    "        total_reaches = 0\n",
    "        proper_reaches = 0\n",
    "        error_frames = []\n",
    "        \n",
    "        # Process each frame\n",
    "        frame_idx = 0\n",
    "        progress_interval = max(1, frame_count // 20)  # Update progress 20 times\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                # Extract pose\n",
    "                pose_data = self.pose_extractor.extract_pose(frame)\n",
    "                all_pose_data.append(pose_data)\n",
    "                \n",
    "                # Calculate angle\n",
    "                angle = self.angle_calculator.calculate_waist_angle(\n",
    "                    pose_data, frame.shape, self.pose_extractor\n",
    "                )\n",
    "                all_angles.append(angle)\n",
    "                \n",
    "                # Detect cycles\n",
    "                cycle_result = self.cycle_detector.process_frame_angle(angle, frame_idx)\n",
    "                \n",
    "                # Update counters\n",
    "                if cycle_result['reach_completed']:\n",
    "                    total_reaches += 1\n",
    "                    if cycle_result['is_proper_reach']:\n",
    "                        proper_reaches += 1\n",
    "                \n",
    "                # Create annotated frame\n",
    "                if video_writer is not None:\n",
    "                    annotated_frame = frame.copy()\n",
    "                    \n",
    "                    # Draw pose landmarks\n",
    "                    annotated_frame = self.video_annotator.draw_pose_landmarks(\n",
    "                        annotated_frame, pose_data\n",
    "                    )\n",
    "                    \n",
    "                    # Draw angle measurement\n",
    "                    annotated_frame = self.video_annotator.draw_angle_measurement(\n",
    "                        annotated_frame, pose_data, angle, self.pose_extractor\n",
    "                    )\n",
    "                    \n",
    "                    # Draw status information\n",
    "                    annotated_frame = self.video_annotator.draw_reach_status(\n",
    "                        annotated_frame, cycle_result, total_reaches, proper_reaches\n",
    "                    )\n",
    "                    \n",
    "                    video_writer.write(annotated_frame)\n",
    "                \n",
    "                # Store frame result\n",
    "                frame_results.append({\n",
    "                    'frame_idx': frame_idx,\n",
    "                    'angle': angle,\n",
    "                    'pose_detected': pose_data is not None,\n",
    "                    'cycle_result': cycle_result\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error processing frame {frame_idx}: {str(e)}\")\n",
    "                error_frames.append(frame_idx)\n",
    "                all_angles.append(None)\n",
    "                all_pose_data.append(None)\n",
    "                frame_results.append({\n",
    "                    'frame_idx': frame_idx,\n",
    "                    'angle': None,\n",
    "                    'pose_detected': False,\n",
    "                    'cycle_result': {'frame_idx': frame_idx, 'angle': None, \n",
    "                                   'new_reach_detected': False, 'reach_completed': False,\n",
    "                                   'is_proper_reach': False, 'cycle_info': None}\n",
    "                })\n",
    "            \n",
    "            frame_idx += 1\n",
    "            \n",
    "            # Progress update\n",
    "            if frame_idx % progress_interval == 0:\n",
    "                progress = (frame_idx / frame_count) * 100\n",
    "                print(f\"   üîÑ Progress: {progress:.1f}% ({frame_idx}/{frame_count} frames)\")\n",
    "        \n",
    "        # Clean up\n",
    "        cap.release()\n",
    "        if video_writer is not None:\n",
    "            video_writer.release()\n",
    "        \n",
    "        # Perform final analysis\n",
    "        print(\"üìà Performing final analysis...\")\n",
    "        \n",
    "        # Smooth angles\n",
    "        smoothed_angles = self.angle_calculator.smooth_angles(\n",
    "            all_angles, self.config.SMOOTHING_WINDOW\n",
    "        )\n",
    "        \n",
    "        # Complete cycle analysis\n",
    "        final_analysis = self.cycle_detector.analyze_complete_sequence(smoothed_angles)\n",
    "        \n",
    "        # Calculate additional statistics\n",
    "        valid_angles = [a for a in smoothed_angles if a is not None]\n",
    "        pose_detection_rate = sum(1 for p in all_pose_data if p is not None) / len(all_pose_data) * 100\n",
    "        \n",
    "        # Compile results\n",
    "        results = {\n",
    "            'video_info': {\n",
    "                'path': video_path,\n",
    "                'fps': fps,\n",
    "                'frame_count': frame_count,\n",
    "                'duration_seconds': frame_count / fps,\n",
    "                'resolution': (width, height)\n",
    "            },\n",
    "            'analysis_results': final_analysis,\n",
    "            'statistics': {\n",
    "                'pose_detection_rate': pose_detection_rate,\n",
    "                'total_valid_angles': len(valid_angles),\n",
    "                'error_frames': len(error_frames),\n",
    "                'error_rate': len(error_frames) / frame_count * 100,\n",
    "                'average_angle': np.mean(valid_angles) if valid_angles else 0,\n",
    "                'angle_std': np.std(valid_angles) if valid_angles else 0,\n",
    "                'min_angle': min(valid_angles) if valid_angles else 0,\n",
    "                'max_angle': max(valid_angles) if valid_angles else 0\n",
    "            },\n",
    "            'data': {\n",
    "                'angles': all_angles,\n",
    "                'smoothed_angles': smoothed_angles,\n",
    "                'frame_results': frame_results,\n",
    "                'error_frames': error_frames\n",
    "            },\n",
    "            'output_files': {\n",
    "                'annotated_video': output_video_path,\n",
    "                'output_directory': output_dir\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save analysis data\n",
    "        if self.config.EXPORT_DATA_CSV:\n",
    "            self._save_data_csv(results, output_dir)\n",
    "        \n",
    "        # Generate and save plots\n",
    "        if self.config.SAVE_ANALYSIS_PLOTS:\n",
    "            self._save_analysis_plots(results, output_dir)\n",
    "        \n",
    "        print(\"‚úÖ Analysis completed successfully!\")\n",
    "        return results\n",
    "    \n",
    "    def _save_data_csv(self, results: Dict, output_dir: str):\n",
    "        \"\"\"Save analysis data to CSV files\"\"\"\n",
    "        # Frame-by-frame data\n",
    "        frame_data = []\n",
    "        for i, (angle, smoothed_angle, frame_result) in enumerate(zip(\n",
    "            results['data']['angles'],\n",
    "            results['data']['smoothed_angles'],\n",
    "            results['data']['frame_results']\n",
    "        )):\n",
    "            frame_data.append({\n",
    "                'frame': i,\n",
    "                'time_seconds': i / results['video_info']['fps'],\n",
    "                'raw_angle': angle,\n",
    "                'smoothed_angle': smoothed_angle,\n",
    "                'pose_detected': frame_result['pose_detected'],\n",
    "                'new_reach_detected': frame_result['cycle_result']['new_reach_detected'],\n",
    "                'reach_completed': frame_result['cycle_result']['reach_completed'],\n",
    "                'is_proper_reach': frame_result['cycle_result']['is_proper_reach']\n",
    "            })\n",
    "        \n",
    "        frame_df = pd.DataFrame(frame_data)\n",
    "        frame_csv_path = os.path.join(output_dir, 'frame_analysis.csv')\n",
    "        frame_df.to_csv(frame_csv_path, index=False)\n",
    "        \n",
    "        # Reach cycles data\n",
    "        if results['analysis_results']['reach_cycles']:\n",
    "            cycles_data = []\n",
    "            for i, cycle in enumerate(results['analysis_results']['reach_cycles']):\n",
    "                cycles_data.append({\n",
    "                    'cycle_number': i + 1,\n",
    "                    'start_frame': cycle['start_frame'],\n",
    "                    'end_frame': cycle['end_frame'],\n",
    "                    'max_angle_frame': cycle['max_angle_frame'],\n",
    "                    'start_time': cycle['start_frame'] / results['video_info']['fps'],\n",
    "                    'end_time': cycle['end_frame'] / results['video_info']['fps'],\n",
    "                    'duration_seconds': cycle['duration'] / results['video_info']['fps'],\n",
    "                    'max_angle': cycle['max_angle'],\n",
    "                    'is_proper_reach': cycle['is_proper_reach']\n",
    "                })\n",
    "            \n",
    "            cycles_df = pd.DataFrame(cycles_data)\n",
    "            cycles_csv_path = os.path.join(output_dir, 'reach_cycles.csv')\n",
    "            cycles_df.to_csv(cycles_csv_path, index=False)\n",
    "        \n",
    "        print(f\"üìÑ Data exported to CSV files in {output_dir}\")\n",
    "    \n",
    "    def _save_analysis_plots(self, results: Dict, output_dir: str):\n",
    "        \"\"\"Generate and save analysis plots\"\"\"\n",
    "        # Angle over time plot\n",
    "        fig = self.video_annotator.create_angle_plot(\n",
    "            results['data']['smoothed_angles'],\n",
    "            results['analysis_results']['reach_cycles'],\n",
    "            results['video_info']['fps']\n",
    "        )\n",
    "        \n",
    "        plot_path = os.path.join(output_dir, 'angle_analysis.png')\n",
    "        fig.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        \n",
    "        # Summary statistics plot\n",
    "        self._create_summary_plot(results, output_dir)\n",
    "        \n",
    "        print(f\"üìä Analysis plots saved to {output_dir}\")\n",
    "    \n",
    "    def _create_summary_plot(self, results: Dict, output_dir: str):\n",
    "        \"\"\"Create summary statistics visualization\"\"\"\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        analysis = results['analysis_results']\n",
    "        stats = results['statistics']\n",
    "        \n",
    "        # Reach success pie chart\n",
    "        if analysis['total_reaches'] > 0:\n",
    "            labels = ['Proper Reaches', 'Improper Reaches']\n",
    "            sizes = [analysis['proper_reaches'], analysis['improper_reaches']]\n",
    "            colors = ['#2ecc71', '#e74c3c']\n",
    "            ax1.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "            ax1.set_title(f'Reach Success Rate\\n(Total: {analysis[\"total_reaches\"]} reaches)')\n",
    "        else:\n",
    "            ax1.text(0.5, 0.5, 'No reaches detected', ha='center', va='center', transform=ax1.transAxes)\n",
    "            ax1.set_title('Reach Success Rate')\n",
    "        \n",
    "        # Angle distribution histogram\n",
    "        valid_angles = [a for a in results['data']['smoothed_angles'] if a is not None]\n",
    "        if valid_angles:\n",
    "            ax2.hist(valid_angles, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            ax2.axvline(config.PROPER_REACH_THRESHOLD, color='green', linestyle='--', \n",
    "                       label=f'Proper Reach Threshold ({config.PROPER_REACH_THRESHOLD}¬∞)')\n",
    "            ax2.axvline(config.MIN_REACH_ANGLE, color='orange', linestyle='--', \n",
    "                       label=f'Min Reach Angle ({config.MIN_REACH_ANGLE}¬∞)')\n",
    "            ax2.set_xlabel('Angle (degrees)')\n",
    "            ax2.set_ylabel('Frequency')\n",
    "            ax2.set_title('Angle Distribution')\n",
    "            ax2.legend()\n",
    "        \n",
    "        # Performance metrics bar chart\n",
    "        metrics = ['Pose Detection Rate', 'Success Rate', 'Error Rate']\n",
    "        values = [stats['pose_detection_rate'], analysis['success_rate'], stats['error_rate']]\n",
    "        colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "        \n",
    "        bars = ax3.bar(metrics, values, color=colors, alpha=0.7)\n",
    "        ax3.set_ylabel('Percentage (%)')\n",
    "        ax3.set_title('Performance Metrics')\n",
    "        ax3.set_ylim(0, 100)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, values):\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                    f'{value:.1f}%', ha='center', va='bottom')\n",
    "        \n",
    "        # Reach cycles timeline\n",
    "        if analysis['reach_cycles']:\n",
    "            y_pos = 0\n",
    "            for i, cycle in enumerate(analysis['reach_cycles']):\n",
    "                start_time = cycle['start_frame'] / results['video_info']['fps']\n",
    "                duration = cycle['duration'] / results['video_info']['fps']\n",
    "                color = '#2ecc71' if cycle['is_proper_reach'] else '#e74c3c'\n",
    "                \n",
    "                ax4.barh(y_pos, duration, left=start_time, height=0.6, \n",
    "                        color=color, alpha=0.7, label='Proper' if cycle['is_proper_reach'] else 'Improper')\n",
    "                \n",
    "                # Add angle label\n",
    "                ax4.text(start_time + duration/2, y_pos, f\"{cycle['max_angle']:.0f}¬∞\", \n",
    "                        ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "                \n",
    "                y_pos += 1\n",
    "            \n",
    "            ax4.set_xlabel('Time (seconds)')\n",
    "            ax4.set_ylabel('Reach Cycle')\n",
    "            ax4.set_title('Reach Cycles Timeline')\n",
    "            ax4.set_ylim(-0.5, len(analysis['reach_cycles']) - 0.5)\n",
    "            \n",
    "            # Create legend\n",
    "            handles = [plt.Rectangle((0,0),1,1, color='#2ecc71', alpha=0.7, label='Proper Reach'),\n",
    "                      plt.Rectangle((0,0),1,1, color='#e74c3c', alpha=0.7, label='Improper Reach')]\n",
    "            ax4.legend(handles=handles)\n",
    "        else:\n",
    "            ax4.text(0.5, 0.5, 'No reach cycles detected', ha='center', va='center', transform=ax4.transAxes)\n",
    "            ax4.set_title('Reach Cycles Timeline')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        summary_plot_path = os.path.join(output_dir, 'summary_analysis.png')\n",
    "        fig.savefig(summary_plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "    \n",
    "    def print_analysis_summary(self, results: Dict):\n",
    "        \"\"\"Print a formatted summary of the analysis results\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìã SIT AND REACH ANALYSIS SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Video info\n",
    "        video_info = results['video_info']\n",
    "        print(f\"üé• Video: {os.path.basename(video_info['path'])}\")\n",
    "        print(f\"   Duration: {video_info['duration_seconds']:.1f} seconds\")\n",
    "        print(f\"   Resolution: {video_info['resolution'][0]}x{video_info['resolution'][1]}\")\n",
    "        print(f\"   Frame Rate: {video_info['fps']:.1f} FPS\")\n",
    "        \n",
    "        # Analysis results\n",
    "        analysis = results['analysis_results']\n",
    "        print(f\"\\nüéØ PERFORMANCE RESULTS:\")\n",
    "        print(f\"   Total Reach Attempts: {analysis['total_reaches']}\")\n",
    "        print(f\"   Proper Reaches: {analysis['proper_reaches']}\")\n",
    "        print(f\"   Improper Reaches: {analysis['improper_reaches']}\")\n",
    "        print(f\"   Success Rate: {analysis['success_rate']:.1f}%\")\n",
    "        print(f\"   Maximum Angle Achieved: {analysis['overall_max_angle']:.1f}¬∞\")\n",
    "        print(f\"   Average Maximum Angle: {analysis['average_max_angle']:.1f}¬∞\")\n",
    "        \n",
    "        # Technical stats\n",
    "        stats = results['statistics']\n",
    "        print(f\"\\nüîß TECHNICAL STATISTICS:\")\n",
    "        print(f\"   Pose Detection Rate: {stats['pose_detection_rate']:.1f}%\")\n",
    "        print(f\"   Error Rate: {stats['error_rate']:.1f}%\")\n",
    "        print(f\"   Valid Angle Measurements: {stats['total_valid_angles']}\")\n",
    "        print(f\"   Average Angle: {stats['average_angle']:.1f}¬∞\")\n",
    "        print(f\"   Angle Range: {stats['min_angle']:.1f}¬∞ - {stats['max_angle']:.1f}¬∞\")\n",
    "        \n",
    "        # Output files\n",
    "        output_files = results['output_files']\n",
    "        print(f\"\\nüìÅ OUTPUT FILES:\")\n",
    "        print(f\"   Output Directory: {output_files['output_directory']}\")\n",
    "        if output_files['annotated_video']:\n",
    "            print(f\"   Annotated Video: {os.path.basename(output_files['annotated_video'])}\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        self.pose_extractor.close()\n",
    "\n",
    "# Initialize the main analyzer\n",
    "analyzer = SitReachAnalyzer(config)\n",
    "print(\"‚úÖ Sit and Reach Analyzer initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf705eb5",
   "metadata": {},
   "source": [
    "## Usage Examples and Demo\n",
    "\n",
    "### How to Use This System\n",
    "\n",
    "1. **Basic Usage**: Analyze a video file\n",
    "2. **Custom Configuration**: Adjust parameters for different scenarios\n",
    "3. **Batch Processing**: Process multiple videos\n",
    "4. **Results Interpretation**: Understanding the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "614a531e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Basic analysis function ready!\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Basic Video Analysis\n",
    "def analyze_sit_reach_video(video_path: str, output_dir: str = \"sit_reach_output\"):\n",
    "    \"\"\"\n",
    "    Simple function to analyze a sit and reach video\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to the video file\n",
    "        output_dir: Directory to save results\n",
    "        \n",
    "    Returns:\n",
    "        Analysis results dictionary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize analyzer\n",
    "        analyzer = SitReachAnalyzer(config)\n",
    "        \n",
    "        # Analyze the video\n",
    "        results = analyzer.analyze_video(video_path, output_dir)\n",
    "        \n",
    "        # Print summary\n",
    "        analyzer.print_analysis_summary(results)\n",
    "        \n",
    "        # Clean up\n",
    "        analyzer.cleanup()\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing video: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (uncomment and modify the path to use)\n",
    "# video_path = \"path/to/your/sit_and_reach_video.mp4\"\n",
    "# results = analyze_sit_reach_video(video_path)\n",
    "\n",
    "print(\"‚úÖ Basic analysis function ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02b244dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Custom configuration functions ready!\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Custom Configuration\n",
    "def create_custom_config(proper_reach_threshold: float = 70.0, \n",
    "                        min_reach_angle: float = 40.0,\n",
    "                        pose_confidence: float = 0.8) -> SitReachConfig:\n",
    "    \"\"\"\n",
    "    Create a custom configuration for specific requirements\n",
    "    \n",
    "    Args:\n",
    "        proper_reach_threshold: Minimum angle for proper reach (degrees)\n",
    "        min_reach_angle: Minimum angle to consider as reach attempt (degrees)\n",
    "        pose_confidence: Confidence threshold for pose detection\n",
    "        \n",
    "    Returns:\n",
    "        Custom SitReachConfig object\n",
    "    \"\"\"\n",
    "    custom_config = SitReachConfig()\n",
    "    custom_config.PROPER_REACH_THRESHOLD = proper_reach_threshold\n",
    "    custom_config.MIN_REACH_ANGLE = min_reach_angle\n",
    "    custom_config.POSE_CONFIDENCE = pose_confidence\n",
    "    \n",
    "    print(f\"üîß Custom configuration created:\")\n",
    "    print(f\"   Proper Reach Threshold: {proper_reach_threshold}¬∞\")\n",
    "    print(f\"   Min Reach Angle: {min_reach_angle}¬∞\")\n",
    "    print(f\"   Pose Confidence: {pose_confidence}\")\n",
    "    \n",
    "    return custom_config\n",
    "\n",
    "def analyze_with_custom_config(video_path: str, custom_config: SitReachConfig):\n",
    "    \"\"\"\n",
    "    Analyze video with custom configuration\n",
    "    \"\"\"\n",
    "    analyzer = SitReachAnalyzer(custom_config)\n",
    "    results = analyzer.analyze_video(video_path, \"custom_output\")\n",
    "    analyzer.print_analysis_summary(results)\n",
    "    analyzer.cleanup()\n",
    "    return results\n",
    "\n",
    "# Example: Create stricter requirements\n",
    "# strict_config = create_custom_config(proper_reach_threshold=80.0, min_reach_angle=50.0)\n",
    "# results = analyze_with_custom_config(\"video.mp4\", strict_config)\n",
    "\n",
    "print(\"‚úÖ Custom configuration functions ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38a970f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch processing functions ready!\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Batch Processing Multiple Videos\n",
    "def batch_analyze_videos(video_paths: List[str], output_base_dir: str = \"batch_output\") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Analyze multiple videos in batch\n",
    "    \n",
    "    Args:\n",
    "        video_paths: List of video file paths\n",
    "        output_base_dir: Base directory for all outputs\n",
    "        \n",
    "    Returns:\n",
    "        List of analysis results for each video\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for i, video_path in enumerate(video_paths):\n",
    "        print(f\"\\nüé¨ Processing video {i+1}/{len(video_paths)}: {os.path.basename(video_path)}\")\n",
    "        \n",
    "        # Create individual output directory\n",
    "        video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        output_dir = os.path.join(output_base_dir, video_name)\n",
    "        \n",
    "        try:\n",
    "            # Analyze video\n",
    "            analyzer = SitReachAnalyzer(config)\n",
    "            results = analyzer.analyze_video(video_path, output_dir)\n",
    "            analyzer.cleanup()\n",
    "            \n",
    "            all_results.append(results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {video_path}: {str(e)}\")\n",
    "            all_results.append(None)\n",
    "    \n",
    "    # Create batch summary\n",
    "    create_batch_summary(all_results, output_base_dir)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def create_batch_summary(all_results: List[Dict], output_dir: str):\n",
    "    \"\"\"Create a summary report for batch processing\"\"\"\n",
    "    valid_results = [r for r in all_results if r is not None]\n",
    "    \n",
    "    if not valid_results:\n",
    "        print(\"‚ùå No valid results to summarize\")\n",
    "        return\n",
    "    \n",
    "    # Compile batch statistics\n",
    "    batch_stats = {\n",
    "        'total_videos': len(all_results),\n",
    "        'successful_analyses': len(valid_results),\n",
    "        'total_reaches': sum(r['analysis_results']['total_reaches'] for r in valid_results),\n",
    "        'total_proper_reaches': sum(r['analysis_results']['proper_reaches'] for r in valid_results),\n",
    "        'average_success_rate': np.mean([r['analysis_results']['success_rate'] for r in valid_results]),\n",
    "        'max_angle_overall': max(r['analysis_results']['overall_max_angle'] for r in valid_results),\n",
    "        'average_pose_detection': np.mean([r['statistics']['pose_detection_rate'] for r in valid_results])\n",
    "    }\n",
    "    \n",
    "    # Print batch summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üìä BATCH ANALYSIS SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Videos Processed: {batch_stats['successful_analyses']}/{batch_stats['total_videos']}\")\n",
    "    print(f\"Total Reach Attempts: {batch_stats['total_reaches']}\")\n",
    "    print(f\"Total Proper Reaches: {batch_stats['total_proper_reaches']}\")\n",
    "    print(f\"Average Success Rate: {batch_stats['average_success_rate']:.1f}%\")\n",
    "    print(f\"Maximum Angle Achieved: {batch_stats['max_angle_overall']:.1f}¬∞\")\n",
    "    print(f\"Average Pose Detection Rate: {batch_stats['average_pose_detection']:.1f}%\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Save batch summary to file\n",
    "    summary_path = os.path.join(output_dir, \"batch_summary.json\")\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(batch_stats, f, indent=2)\n",
    "    \n",
    "    print(f\"üìÑ Batch summary saved to: {summary_path}\")\n",
    "\n",
    "# Example usage:\n",
    "# video_list = [\"video1.mp4\", \"video2.mp4\", \"video3.mp4\"]\n",
    "# batch_results = batch_analyze_videos(video_list)\n",
    "\n",
    "print(\"‚úÖ Batch processing functions ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b52c9e",
   "metadata": {},
   "source": [
    "### Quick Start Guide\n",
    "\n",
    "**To use this system with your own video:**\n",
    "\n",
    "1. **Place your video file** in an accessible location\n",
    "2. **Update the video path** in the example below\n",
    "3. **Run the analysis** and review results\n",
    "\n",
    "**Required video format:**\n",
    "- Common formats: MP4, AVI, MOV\n",
    "- Clear view of the person performing sit and reach\n",
    "- Person should be visible from the side for best angle measurement\n",
    "- Good lighting and minimal background clutter recommended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "480d7796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # üöÄ READY TO USE - UNCOMMENT AND MODIFY THE PATH BELOW\n",
    "\n",
    "# # Step 1: Set your video path\n",
    "# video_path = \"C:\\Users\\Yash\\Documents\\GitHub\\KhelSetu\\sit_up.mp4\"\n",
    "\n",
    "# # Step 2: Run the analysis\n",
    "# print(\"üé¨ Starting Sit and Reach Analysis...\")\n",
    "# results = analyze_sit_reach_video(video_path, \"my_analysis_output\")\n",
    "\n",
    "# # Step 3: Review results\n",
    "# if results:\n",
    "#     print(\"\\\\n‚úÖ Analysis completed! Check the output folder for:\")\n",
    "#     print(\"   üìπ Annotated video with pose landmarks and measurements\")\n",
    "#     print(\"   üìä Analysis plots and charts\")  \n",
    "#     print(\"   üìÑ CSV files with detailed data\")\n",
    "#     print(\"   üìà Summary statistics and performance metrics\")\n",
    "# else:\n",
    "#     print(\"‚ùå Analysis failed. Please check your video path and format.\")\n",
    "\n",
    "# # Alternative: Test with a demo (if you have OpenCV test data)\n",
    "# def test_with_sample_data():\n",
    "#     \"\"\"\n",
    "#     Test the system with generated sample data\n",
    "#     This is useful for testing without a video file\n",
    "#     \"\"\"\n",
    "#     print(\"üß™ Testing system with sample data...\")\n",
    "    \n",
    "#     # Generate sample angle data that simulates sit and reach movements\n",
    "#     sample_angles = []\n",
    "#     for i in range(300):  # 10 seconds at 30 FPS\n",
    "#         base_angle = 30\n",
    "#         if 50 <= i <= 80:  # First reach\n",
    "#             reach_angle = base_angle + 30 * np.sin((i - 50) * np.pi / 30)\n",
    "#         elif 120 <= i <= 150:  # Second reach  \n",
    "#             reach_angle = base_angle + 45 * np.sin((i - 120) * np.pi / 30)\n",
    "#         elif 200 <= i <= 230:  # Third reach\n",
    "#             reach_angle = base_angle + 35 * np.sin((i - 200) * np.pi / 30)\n",
    "#         else:\n",
    "#             reach_angle = base_angle + np.random.normal(0, 2)  # Small random variation\n",
    "        \n",
    "#         sample_angles.append(max(0, reach_angle))\n",
    "    \n",
    "#     # Test cycle detection\n",
    "#     cycle_detector_test = CycleDetector()\n",
    "#     test_analysis = cycle_detector_test.analyze_complete_sequence(sample_angles)\n",
    "    \n",
    "#     print(f\"üìä Sample Data Analysis Results:\")\n",
    "#     print(f\"   Total reaches detected: {test_analysis['total_reaches']}\")\n",
    "#     print(f\"   Proper reaches: {test_analysis['proper_reaches']}\")\n",
    "#     print(f\"   Success rate: {test_analysis['success_rate']:.1f}%\")\n",
    "#     print(f\"   Max angle: {test_analysis['overall_max_angle']:.1f}¬∞\")\n",
    "    \n",
    "#     # Create a simple plot\n",
    "#     fig, ax = plt.subplots(figsize=(12, 6))\n",
    "#     ax.plot(sample_angles, 'b-', linewidth=2, label='Simulated Waist Angle')\n",
    "#     ax.axhline(y=config.PROPER_REACH_THRESHOLD, color='green', linestyle='--', \n",
    "#                label=f'Proper Reach Threshold ({config.PROPER_REACH_THRESHOLD}¬∞)')\n",
    "#     ax.set_xlabel('Frame')\n",
    "#     ax.set_ylabel('Angle (degrees)')\n",
    "#     ax.set_title('Sample Sit and Reach Data')\n",
    "#     ax.legend()\n",
    "#     ax.grid(True, alpha=0.3)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     return test_analysis\n",
    "\n",
    "# # Uncomment to test with sample data\n",
    "# # test_results = test_with_sample_data()\n",
    "\n",
    "# print(\"üéØ System is ready! Uncomment the example above to start analyzing your videos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b75e669",
   "metadata": {},
   "source": [
    "## Installation Requirements\n",
    "\n",
    "To run this notebook, you'll need to install the following packages:\n",
    "\n",
    "```bash\n",
    "pip install opencv-python mediapipe numpy matplotlib pandas\n",
    "```\n",
    "\n",
    "**For optimal performance:**\n",
    "```bash\n",
    "pip install opencv-contrib-python  # Additional OpenCV modules\n",
    "pip install scikit-learn           # For advanced analysis (optional)\n",
    "```\n",
    "\n",
    "## System Features Summary\n",
    "\n",
    "‚úÖ **Pose Detection**: MediaPipe Pose with high accuracy settings  \n",
    "‚úÖ **Angle Measurement**: Precise waist/torso angle calculation  \n",
    "‚úÖ **Cycle Detection**: Automatic reach attempt counting  \n",
    "‚úÖ **Threshold Validation**: Configurable proper reach criteria  \n",
    "‚úÖ **Video Annotation**: Real-time pose landmarks and measurements  \n",
    "‚úÖ **Comprehensive Analysis**: Detailed statistics and insights  \n",
    "‚úÖ **Multiple Output Formats**: CSV data, plots, annotated video  \n",
    "‚úÖ **Batch Processing**: Handle multiple videos efficiently  \n",
    "‚úÖ **Error Handling**: Robust error detection and reporting  \n",
    "‚úÖ **Customizable**: Adjustable parameters for different scenarios\n",
    "\n",
    "## Output Files Generated\n",
    "\n",
    "1. **`annotated_video.mp4`** - Video with pose landmarks and measurements\n",
    "2. **`angle_analysis.png`** - Time series plot of angle measurements  \n",
    "3. **`summary_analysis.png`** - Comprehensive statistics dashboard\n",
    "4. **`frame_analysis.csv`** - Frame-by-frame detailed data\n",
    "5. **`reach_cycles.csv`** - Individual reach cycle information\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ This system is ready for production use in fitness assessment applications!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dd59eb",
   "metadata": {},
   "source": [
    "## üîß Fix for Windows File Paths\n",
    "\n",
    "**Important:** When using Windows file paths in Python, you need to handle backslashes properly. Here are three ways to fix the path issue:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8a71b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Video path set to: C:/Users/Yash/Documents/GitHub/KhelSetu/Sit_and_Reach_Test_Animation_Generated.mp4\n",
      "üìÅ File exists: True\n",
      "üé¨ Starting Sit and Reach Analysis...\n",
      "üìπ Processing video: C:/Users/Yash/Documents/GitHub/KhelSetu/Sit_and_Reach_Test_Animation_Generated.mp4\n",
      "   üìä Properties: 1280x720, 24.0 FPS, 187 frames\n",
      "   ‚è±Ô∏è Duration: 7.8 seconds\n",
      "   üîÑ Progress: 4.8% (9/187 frames)\n",
      "   üîÑ Progress: 9.6% (18/187 frames)\n",
      "   üîÑ Progress: 14.4% (27/187 frames)\n",
      "   üîÑ Progress: 19.3% (36/187 frames)\n",
      "   üîÑ Progress: 24.1% (45/187 frames)\n",
      "   üîÑ Progress: 28.9% (54/187 frames)\n",
      "   üîÑ Progress: 33.7% (63/187 frames)\n",
      "   üîÑ Progress: 38.5% (72/187 frames)\n",
      "   üîÑ Progress: 43.3% (81/187 frames)\n",
      "   üîÑ Progress: 48.1% (90/187 frames)\n",
      "   üîÑ Progress: 52.9% (99/187 frames)\n",
      "   üîÑ Progress: 57.8% (108/187 frames)\n",
      "   üîÑ Progress: 62.6% (117/187 frames)\n",
      "   üîÑ Progress: 67.4% (126/187 frames)\n",
      "   üîÑ Progress: 72.2% (135/187 frames)\n",
      "   üîÑ Progress: 77.0% (144/187 frames)\n",
      "   üîÑ Progress: 81.8% (153/187 frames)\n",
      "   üîÑ Progress: 86.6% (162/187 frames)\n",
      "   üîÑ Progress: 91.4% (171/187 frames)\n",
      "   üîÑ Progress: 96.3% (180/187 frames)\n",
      "üìà Performing final analysis...\n",
      "üìÑ Data exported to CSV files in sit_reach_analysis_output2\n",
      "üìä Analysis plots saved to sit_reach_analysis_output2\n",
      "‚úÖ Analysis completed successfully!\n",
      "\n",
      "============================================================\n",
      "üìã SIT AND REACH ANALYSIS SUMMARY\n",
      "============================================================\n",
      "üé• Video: Sit_and_Reach_Test_Animation_Generated.mp4\n",
      "   Duration: 7.8 seconds\n",
      "   Resolution: 1280x720\n",
      "   Frame Rate: 24.0 FPS\n",
      "\n",
      "üéØ PERFORMANCE RESULTS:\n",
      "   Total Reach Attempts: 0\n",
      "   Proper Reaches: 0\n",
      "   Improper Reaches: 0\n",
      "   Success Rate: 0.0%\n",
      "   Maximum Angle Achieved: 0.0¬∞\n",
      "   Average Maximum Angle: 0.0¬∞\n",
      "\n",
      "üîß TECHNICAL STATISTICS:\n",
      "   Pose Detection Rate: 100.0%\n",
      "   Error Rate: 0.0%\n",
      "   Valid Angle Measurements: 187\n",
      "   Average Angle: 63.3¬∞\n",
      "   Angle Range: 45.5¬∞ - 89.6¬∞\n",
      "\n",
      "üìÅ OUTPUT FILES:\n",
      "   Output Directory: sit_reach_analysis_output2\n",
      "   Annotated Video: annotated_Sit_and_Reach_Test_Animation_Generated.mp4\n",
      "============================================================\n",
      "\n",
      "‚úÖ Analysis completed! Check the output folder for:\n",
      "   üìπ Annotated video with pose landmarks and measurements\n",
      "   üìä Analysis plots and charts\n",
      "   üìÑ CSV files with detailed data\n",
      "   üìà Summary statistics and performance metrics\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Method 1: Use forward slashes (recommended - works on all platforms)\n",
    "video_path = \"C:/Users/Yash/Documents/GitHub/KhelSetu/Sit_and_Reach_Test_Animation_Generated.mp4\"\n",
    "\n",
    "import os\n",
    "\n",
    "print(f\"‚úÖ Video path set to: {video_path}\")\n",
    "print(f\"üìÅ File exists: {os.path.exists(video_path)}\")\n",
    "\n",
    "# Now run the analysis\n",
    "print(\"üé¨ Starting Sit and Reach Analysis...\")\n",
    "results = analyze_sit_reach_video(video_path, \"sit_reach_analysis_output2\")\n",
    "\n",
    "# Review results\n",
    "if results:\n",
    "    print(\"\\n‚úÖ Analysis completed! Check the output folder for:\")\n",
    "    print(\"   üìπ Annotated video with pose landmarks and measurements\")\n",
    "    print(\"   üìä Analysis plots and charts\")  \n",
    "    print(\"   üìÑ CSV files with detailed data\")\n",
    "    print(\"   üìà Summary statistics and performance metrics\")\n",
    "else:\n",
    "    print(\"‚ùå Analysis failed. Please check your video path and format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639ef412",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6a1d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a072406e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed64817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
