},
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN EXECUTION SECTION - Complete Analysis Pipeline\n",
    "def analyze_sit_reach_video(video_path: str, \n",
    "                          proper_reach_threshold: float = 45.0,\n",
    "                          generate_full_report: bool = True,\n",
    "                          save_annotated_video: bool = True):\n",
    "    \"\"\"\n",
    "    Complete analysis pipeline for sit and reach fitness test\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to the input video file\n",
    "        proper_reach_threshold: Minimum angle for proper reach (degrees)\n",
    "        generate_full_report: Whether to generate comprehensive report\n",
    "        save_annotated_video: Whether to save annotated output video\n",
    "    \n",
    "    Returns:\n",
    "        AnalysisResults object\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"           SIT AND REACH VIDEO ANALYZER\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Input Video: {video_path}\")\n",
    "    print(f\"Proper Reach Threshold: {proper_reach_threshold}°\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = SitReachAnalyzer(\n",
    "        proper_reach_threshold=proper_reach_threshold,\n",
    "        min_cycle_duration=15,\n",
    "        confidence_threshold=0.7\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Process video\n",
    "        results = analyzer.process_video(\n",
    "            video_path=video_path,\n",
    "            save_annotated_video=save_annotated_video\n",
    "        )\n",
    "        \n",
    "        # Display basic results\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"                   QUICK RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total Reach Attempts: {results.total_reach_attempts}\")\n",
    "        print(f\"Proper Reaches: {results.proper_reaches}\")\n",
    "        print(f\"Success Rate: {(results.proper_reaches/max(results.total_reach_attempts,1)*100):.1f}%\")\n",
    "        print(f\"Maximum Angle: {results.max_angle_overall:.1f}°\")\n",
    "        print(f\"Average Angle: {results.average_angle:.1f}°\")\n",
    "        print(f\"Video Quality: {(results.valid_frames/results.total_frames*100):.1f}% valid frames\")\n",
    "        \n",
    "        if generate_full_report:\n",
    "            print(\"\\nGenerating visualizations and detailed report...\")\n",
    "            \n",
    "            # Create visualizations\n",
    "            visualize_analysis_results(results)\n",
    "            visualize_angle_distribution(results)\n",
    "            \n",
    "            # Generate and display detailed report\n",
    "            report = generate_detailed_report(results, video_path)\n",
    "            print(report)\n",
    "            \n",
    "            # Save report to file\n",
    "            report_filename = video_path.replace('.mp4', '_analysis_report.txt')\n",
    "            with open(report_filename, 'w') as f:\n",
    "                f.write(report)\n",
    "            print(f\"\\nDetailed report saved: {report_filename}\")\n",
    "            \n",
    "            # Save results to JSON\n",
    "            json_filename = video_path.replace('.mp4', '_analysis_data.json')\n",
    "            save_results_to_json(results, json_filename)\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Demo function for testing with sample data\n",
    "def create_sample_data_for_demo():\n",
    "    \"\"\"\n",
    "    Create sample analysis results for demonstration purposes\n",
    "    (Use this if you don't have a video file ready)\n",
    "    \"\"\"\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    \n",
    "    # Generate synthetic angle data\n",
    "    np.random.seed(42)\n",
    "    frames = 300\n",
    "    \n",
    "    # Simulate angle data with reach cycles\n",
    "    angle_data = []\n",
    "    base_angle = 15  # Starting position\n",
    "    \n",
    "    for i in range(frames):\n",
    "        # Create reach cycles every 50 frames\n",
    "        cycle_position = i % 50\n",
    "        if cycle_position < 20:\n",
    "            # Reaching phase\n",
    "            reach_progress = cycle_position / 20\n",
    "            max_reach = 50 + np.random.normal(0, 5)  # Variable reach depth\n",
    "            angle = base_angle + (max_reach - base_angle) * reach_progress\n",
    "        else:\n",
    "            # Return phase\n",
    "            return_progress = (cycle_position - 20) / 30\n",
    "            angle = 50 - (50 - base_angle) * return_progress + np.random.normal(0, 2)\n",
    "        \n",
    "        angle_data.append(max(10, min(70, angle)))  # Clamp to realistic range\n",
    "    \n",
    "    confidence_data = [0.9 + np.random.normal(0, 0.05) for _ in range(frames)]\n",
    "    confidence_data = [max(0.5, min(1.0, c)) for c in confidence_data]\n",
    "    \n",
    "    # Create sample reach cycles\n",
    "    reach_cycles = [\n",
    "        ReachCycle(0, 20, 52.3, 15.2, True, 0.91),\n",
    "        ReachCycle(50, 70, 48.7, 14.8, True, 0.88),\n",
    "        ReachCycle(100, 120, 41.2, 15.5, False, 0.85),\n",
    "        ReachCycle(150, 170, 55.1, 14.9, True, 0.92),\n",
    "        ReachCycle(200, 220, 49.8, 15.1, True, 0.89),\n",
    "        ReachCycle(250, 270, 38.4, 16.0, False, 0.82)\n",
    "    ]\n",
    "    \n",
    "    results = AnalysisResults(\n",
    "        total_frames=frames,\n",
    "        valid_frames=int(frames * 0.95),\n",
    "        total_reach_attempts=6,\n",
    "        proper_reaches=4,\n",
    "        max_angle_overall=max(angle_data),\n",
    "        average_angle=np.mean(angle_data),\n",
    "        reach_cycles=reach_cycles,\n",
    "        angle_data=angle_data,\n",
    "        confidence_data=confidence_data,\n",
    "        error_frames=[45, 95, 145, 195, 245, 295]  # Some error frames\n",
    "    )\n",
    "    \n",
    "    print(\"Sample data created! Running demonstration...\")\n",
    "    \n",
    "    # Visualize sample results\n",
    "    visualize_analysis_results(results)\n",
    "    visualize_angle_distribution(results)\n",
    "    \n",
    "    # Generate sample report\n",
    "    report = generate_detailed_report(results, \"sample_sit_reach_video.mp4\")\n",
    "    print(report)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Main execution functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER CONFIGURATION SECTION\n",
    "# =========================\n",
    "# Modify these variables to customize your analysis\n",
    "\n",
    "# VIDEO SETTINGS\n",
    "VIDEO_PATH = \"sit_and_reach_test.mp4\"  # Change this to your video file path\n",
    "PROPER_REACH_THRESHOLD = 45.0  # Minimum angle (degrees) for a proper reach\n",
    "SAVE_ANNOTATED_VIDEO = True   # Set to False if you don't want annotated video output\n",
    "GENERATE_FULL_REPORT = True   # Set to False for quick analysis only\n",
    "\n",
    "# ANALYSIS PARAMETERS (Advanced users)\n",
    "MIN_CYCLE_DURATION = 15      # Minimum frames for a valid reach cycle\n",
    "CONFIDENCE_THRESHOLD = 0.7   # Minimum pose detection confidence\n",
    "\n",
    "# RUN DEMO MODE (if you don't have a video file yet)\n",
    "RUN_DEMO_MODE = False  # Set to True to run{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sit and Reach Fitness Test Video Analyzer\n",
    "\n",
    "This notebook analyzes videos of athletes performing the \"Sit and Reach\" fitness test using MediaPipe Pose estimation.\n",
    "\n",
    "## Features:\n",
    "- Real-time pose detection and keypoint extraction\n",
    "- Accurate waist/torso angle measurement\n",
    "- Automatic counting of proper reach cycles\n",
    "- Visual feedback with annotated frames\n",
    "- Comprehensive analysis reports\n",
    "- Error detection and robustness handling\n",
    "\n",
    "## Requirements:\n",
    "- Python 3.7+\n",
    "- OpenCV\n",
    "- MediaPipe\n",
    "- NumPy\n",
    "- Matplotlib\n",
    "- Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_packages():\n",
    "    packages = [\n",
    "        'opencv-python',\n",
    "        'mediapipe',\n",
    "        'numpy',\n",
    "        'matplotlib',\n",
    "        'pandas',\n",
    "        'seaborn'\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package.replace('-', '_'))\n",
    "            print(f\"✓ {package} already installed\")\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "            print(f\"✓ {package} installed successfully\")\n",
    "\n",
    "install_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up matplotlib for inline plotting\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ReachCycle:\n",
    "    \"\"\"Data class to store information about each reach cycle\"\"\"\n",
    "    start_frame: int\n",
    "    end_frame: int\n",
    "    max_angle: float\n",
    "    min_angle: float\n",
    "    is_proper_reach: bool\n",
    "    confidence_score: float\n",
    "\n",
    "@dataclass\n",
    "class AnalysisResults:\n",
    "    \"\"\"Data class to store complete analysis results\"\"\"\n",
    "    total_frames: int\n",
    "    valid_frames: int\n",
    "    total_reach_attempts: int\n",
    "    proper_reaches: int\n",
    "    max_angle_overall: float\n",
    "    average_angle: float\n",
    "    reach_cycles: List[ReachCycle]\n",
    "    angle_data: List[float]\n",
    "    confidence_data: List[float]\n",
    "    error_frames: List[int]\n",
    "\n",
    "class SitReachAnalyzer:\n",
    "    \"\"\"Main class for analyzing Sit and Reach fitness test videos\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 proper_reach_threshold: float = 45.0,\n",
    "                 min_cycle_duration: int = 15,\n",
    "                 confidence_threshold: float = 0.7):\n",
    "        \"\"\"\n",
    "        Initialize the Sit and Reach Analyzer\n",
    "        \n",
    "        Args:\n",
    "            proper_reach_threshold: Minimum angle (degrees) for a proper reach\n",
    "            min_cycle_duration: Minimum frames for a valid reach cycle\n",
    "            confidence_threshold: Minimum pose confidence for valid detection\n",
    "        \"\"\"\n",
    "        self.proper_reach_threshold = proper_reach_threshold\n",
    "        self.min_cycle_duration = min_cycle_duration\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        \n",
    "        # Initialize MediaPipe Pose\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.pose = self.mp_pose.Pose(\n",
    "            static_image_mode=False,\n",
    "            model_complexity=2,\n",
    "            enable_segmentation=False,\n",
    "            min_detection_confidence=0.7,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        \n",
    "        # Data storage\n",
    "        self.angles = []\n",
    "        self.confidences = []\n",
    "        self.reach_cycles = []\n",
    "        self.error_frames = []\n",
    "        \n",
    "    def calculate_angle(self, point1: Tuple[float, float], \n",
    "                       point2: Tuple[float, float], \n",
    "                       point3: Tuple[float, float]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate angle between three points\n",
    "        \n",
    "        Args:\n",
    "            point1: First point (x, y)\n",
    "            point2: Vertex point (x, y)\n",
    "            point3: Third point (x, y)\n",
    "            \n",
    "        Returns:\n",
    "            Angle in degrees\n",
    "        \"\"\"\n",
    "        # Convert to numpy arrays\n",
    "        p1 = np.array(point1)\n",
    "        p2 = np.array(point2)\n",
    "        p3 = np.array(point3)\n",
    "        \n",
    "        # Calculate vectors\n",
    "        v1 = p1 - p2\n",
    "        v2 = p3 - p2\n",
    "        \n",
    "        # Calculate angle using dot product\n",
    "        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "        cos_angle = np.clip(cos_angle, -1.0, 1.0)  # Ensure valid range\n",
    "        angle = np.arccos(cos_angle)\n",
    "        \n",
    "        return np.degrees(angle)\n",
    "    \n",
    "    def extract_pose_landmarks(self, frame: np.ndarray) -> Tuple[Optional[Dict], float]:\n",
    "        \"\"\"\n",
    "        Extract pose landmarks from a frame\n",
    "        \n",
    "        Args:\n",
    "            frame: Input video frame\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (landmarks dictionary, confidence score)\n",
    "        \"\"\"\n",
    "        # Convert BGR to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process the frame\n",
    "        results = self.pose.process(rgb_frame)\n",
    "        \n",
    "        if not results.pose_landmarks:\n",
    "            return None, 0.0\n",
    "        \n",
    "        # Extract relevant landmarks for sit and reach analysis\n",
    "        landmarks = {}\n",
    "        h, w = frame.shape[:2]\n",
    "        \n",
    "        # Key landmarks for torso angle calculation\n",
    "        landmark_indices = {\n",
    "            'left_shoulder': self.mp_pose.PoseLandmark.LEFT_SHOULDER,\n",
    "            'right_shoulder': self.mp_pose.PoseLandmark.RIGHT_SHOULDER,\n",
    "            'left_hip': self.mp_pose.PoseLandmark.LEFT_HIP,\n",
    "            'right_hip': self.mp_pose.PoseLandmark.RIGHT_HIP,\n",
    "            'left_knee': self.mp_pose.PoseLandmark.LEFT_KNEE,\n",
    "            'right_knee': self.mp_pose.PoseLandmark.RIGHT_KNEE,\n",
    "            'nose': self.mp_pose.PoseLandmark.NOSE\n",
    "        }\n",
    "        \n",
    "        total_confidence = 0\n",
    "        valid_landmarks = 0\n",
    "        \n",
    "        for name, idx in landmark_indices.items():\n",
    "            landmark = results.pose_landmarks.landmark[idx]\n",
    "            if landmark.visibility > 0.5:  # Only use visible landmarks\n",
    "                landmarks[name] = {\n",
    "                    'x': int(landmark.x * w),\n",
    "                    'y': int(landmark.y * h),\n",
    "                    'visibility': landmark.visibility\n",
    "                }\n",
    "                total_confidence += landmark.visibility\n",
    "                valid_landmarks += 1\n",
    "        \n",
    "        avg_confidence = total_confidence / max(valid_landmarks, 1)\n",
    "        \n",
    "        return landmarks, avg_confidence\n",
    "    \n",
    "    def calculate_torso_angle(self, landmarks: Dict) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Calculate the torso flexion angle (waist angle)\n",
    "        \n",
    "        Args:\n",
    "            landmarks: Dictionary of pose landmarks\n",
    "            \n",
    "        Returns:\n",
    "            Torso angle in degrees, or None if calculation fails\n",
    "        \"\"\"\n",
    "        required_points = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
    "        \n",
    "        if not all(point in landmarks for point in required_points):\n",
    "            return None\n",
    "        \n",
    "        # Calculate midpoints\n",
    "        shoulder_mid = (\n",
    "            (landmarks['left_shoulder']['x'] + landmarks['right_shoulder']['x']) / 2,\n",
    "            (landmarks['left_shoulder']['y'] + landmarks['right_shoulder']['y']) / 2\n",
    "        )\n",
    "        \n",
    "        hip_mid = (\n",
    "            (landmarks['left_hip']['x'] + landmarks['right_hip']['x']) / 2,\n",
    "            (landmarks['left_hip']['y'] + landmarks['right_hip']['y']) / 2\n",
    "        )\n",
    "        \n",
    "        # For sit and reach, we need knee reference for angle calculation\n",
    "        if 'left_knee' in landmarks and 'right_knee' in landmarks:\n",
    "            knee_mid = (\n",
    "                (landmarks['left_knee']['x'] + landmarks['right_knee']['x']) / 2,\n",
    "                (landmarks['left_knee']['y'] + landmarks['right_knee']['y']) / 2\n",
    "            )\n",
    "        else:\n",
    "            # Estimate knee position if not detected\n",
    "            knee_mid = (hip_mid[0], hip_mid[1] + 100)  # Approximate knee position\n",
    "        \n",
    "        # Calculate torso flexion angle (angle between torso and vertical)\n",
    "        # Use knee-hip-shoulder angle to measure forward lean\n",
    "        angle = self.calculate_angle(knee_mid, hip_mid, shoulder_mid)\n",
    "        \n",
    "        # Convert to flexion angle (0° = upright, 90° = fully forward)\n",
    "        flexion_angle = abs(angle - 90)\n",
    "        \n",
    "        return flexion_angle\n",
    "    \n",
    "    def detect_reach_cycles(self, angles: List[float], \n",
    "                           confidences: List[float]) -> List[ReachCycle]:\n",
    "        \"\"\"\n",
    "        Detect and count reach cycles from angle data\n",
    "        \n",
    "        Args:\n",
    "            angles: List of torso angles over time\n",
    "            confidences: List of confidence scores\n",
    "            \n",
    "        Returns:\n",
    "            List of detected reach cycles\n",
    "        \"\"\"\n",
    "        if len(angles) < self.min_cycle_duration:\n",
    "            return []\n",
    "        \n",
    "        # Smooth the angle data to reduce noise\n",
    "        window_size = min(5, len(angles))\n",
    "        smoothed_angles = np.convolve(angles, np.ones(window_size)/window_size, mode='valid')\n",
    "        \n",
    "        cycles = []\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(smoothed_angles) - self.min_cycle_duration:\n",
    "            # Look for increase in angle (forward reach)\n",
    "            start_angle = smoothed_angles[i]\n",
    "            max_angle = start_angle\n",
    "            max_idx = i\n",
    "            \n",
    "            # Find peak angle\n",
    "            j = i + 1\n",
    "            while j < len(smoothed_angles) and (j - i) < 60:  # Max 60 frames per cycle\n",
    "                if smoothed_angles[j] > max_angle:\n",
    "                    max_angle = smoothed_angles[j]\n",
    "                    max_idx = j\n",
    "                elif smoothed_angles[j] < max_angle * 0.8:  # Significant decrease\n",
    "                    break\n",
    "                j += 1\n",
    "            \n",
    "            # Validate cycle\n",
    "            cycle_duration = max_idx - i\n",
    "            if cycle_duration >= self.min_cycle_duration and max_angle > start_angle + 5:\n",
    "                # Calculate average confidence for this cycle\n",
    "                cycle_confidence = np.mean(confidences[i:max_idx+1]) if confidences else 0.8\n",
    "                \n",
    "                cycle = ReachCycle(\n",
    "                    start_frame=i,\n",
    "                    end_frame=max_idx,\n",
    "                    max_angle=max_angle,\n",
    "                    min_angle=start_angle,\n",
    "                    is_proper_reach=max_angle >= self.proper_reach_threshold,\n",
    "                    confidence_score=cycle_confidence\n",
    "                )\n",
    "                cycles.append(cycle)\n",
    "                \n",
    "                i = max_idx + self.min_cycle_duration  # Skip ahead to avoid overlaps\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        return cycles\n",
    "    \n",
    "    def annotate_frame(self, frame: np.ndarray, landmarks: Dict, \n",
    "                      angle: float, frame_num: int, is_proper_reach: bool = False) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Annotate frame with pose landmarks and angle information\n",
    "        \n",
    "        Args:\n",
    "            frame: Input frame\n",
    "            landmarks: Pose landmarks\n",
    "            angle: Calculated torso angle\n",
    "            frame_num: Current frame number\n",
    "            is_proper_reach: Whether this is a proper reach\n",
    "            \n",
    "        Returns:\n",
    "            Annotated frame\n",
    "        \"\"\"\n",
    "        annotated_frame = frame.copy()\n",
    "        \n",
    "        # Draw pose landmarks\n",
    "        for name, point in landmarks.items():\n",
    "            color = (0, 255, 0) if point['visibility'] > 0.7 else (0, 255, 255)\n",
    "            cv2.circle(annotated_frame, (point['x'], point['y']), 5, color, -1)\n",
    "            cv2.putText(annotated_frame, name[:3], (point['x']+10, point['y']), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.3, color, 1)\n",
    "        \n",
    "        # Draw torso line if key points are available\n",
    "        if 'left_shoulder' in landmarks and 'right_shoulder' in landmarks:\n",
    "            if 'left_hip' in landmarks and 'right_hip' in landmarks:\n",
    "                # Shoulder midpoint\n",
    "                shoulder_mid = (\n",
    "                    int((landmarks['left_shoulder']['x'] + landmarks['right_shoulder']['x']) / 2),\n",
    "                    int((landmarks['left_shoulder']['y'] + landmarks['right_shoulder']['y']) / 2)\n",
    "                )\n",
    "                \n",
    "                # Hip midpoint\n",
    "                hip_mid = (\n",
    "                    int((landmarks['left_hip']['x'] + landmarks['right_hip']['x']) / 2),\n",
    "                    int((landmarks['left_hip']['y'] + landmarks['right_hip']['y']) / 2)\n",
    "                )\n",
    "                \n",
    "                # Draw torso line\n",
    "                line_color = (0, 255, 0) if is_proper_reach else (255, 255, 0)\n",
    "                cv2.line(annotated_frame, shoulder_mid, hip_mid, line_color, 3)\n",
    "        \n",
    "        # Add text annotations\n",
    "        text_color = (0, 255, 0) if is_proper_reach else (255, 255, 255)\n",
    "        cv2.putText(annotated_frame, f'Frame: {frame_num}', (10, 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, text_color, 2)\n",
    "        cv2.putText(annotated_frame, f'Torso Angle: {angle:.1f}°', (10, 60), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, text_color, 2)\n",
    "        \n",
    "        if is_proper_reach:\n",
    "            cv2.putText(annotated_frame, 'PROPER REACH!', (10, 90), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw threshold line indicator\n",
    "        threshold_text = f'Threshold: {self.proper_reach_threshold}°'\n",
    "        cv2.putText(annotated_frame, threshold_text, (10, frame.shape[0]-20), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        \n",
    "        return annotated_frame\n",
    "    \n",
    "    def process_video(self, video_path: str, \n",
    "                     output_path: Optional[str] = None, \n",
    "                     save_annotated_video: bool = True) -> AnalysisResults:\n",
    "        \"\"\"\n",
    "        Process the entire video and analyze sit and reach performance\n",
    "        \n",
    "        Args:\n",
    "            video_path: Path to input video file\n",
    "            output_path: Path for output annotated video\n",
    "            save_annotated_video: Whether to save annotated video\n",
    "            \n",
    "        Returns:\n",
    "            AnalysisResults object containing all analysis data\n",
    "        \"\"\"\n",
    "        if not os.path.exists(video_path):\n",
    "            raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "        \n",
    "        # Reset data\n",
    "        self.angles = []\n",
    "        self.confidences = []\n",
    "        self.error_frames = []\n",
    "        \n",
    "        # Open video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        \n",
    "        print(f\"Processing video: {video_path}\")\n",
    "        print(f\"Total frames: {total_frames}, FPS: {fps}, Size: {width}x{height}\")\n",
    "        \n",
    "        # Setup output video writer if needed\n",
    "        if save_annotated_video:\n",
    "            if output_path is None:\n",
    "                output_path = video_path.replace('.mp4', '_analyzed.mp4')\n",
    "            \n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        frame_count = 0\n",
    "        valid_frames = 0\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # Extract pose landmarks\n",
    "                landmarks, confidence = self.extract_pose_landmarks(frame)\n",
    "                \n",
    "                if landmarks and confidence >= self.confidence_threshold:\n",
    "                    # Calculate torso angle\n",
    "                    angle = self.calculate_torso_angle(landmarks)\n",
    "                    \n",
    "                    if angle is not None:\n",
    "                        self.angles.append(angle)\n",
    "                        self.confidences.append(confidence)\n",
    "                        valid_frames += 1\n",
    "                        \n",
    "                        # Check if this is a proper reach\n",
    "                        is_proper = angle >= self.proper_reach_threshold\n",
    "                        \n",
    "                        # Annotate frame\n",
    "                        if save_annotated_video:\n",
    "                            annotated_frame = self.annotate_frame(\n",
    "                                frame, landmarks, angle, frame_count, is_proper\n",
    "                            )\n",
    "                            out.write(annotated_frame)\n",
    "                    else:\n",
    "                        self.error_frames.append(frame_count)\n",
    "                        if save_annotated_video:\n",
    "                            out.write(frame)\n",
    "                else:\n",
    "                    self.error_frames.append(frame_count)\n",
    "                    if save_annotated_video:\n",
    "                        # Add error annotation\n",
    "                        error_frame = frame.copy()\n",
    "                        cv2.putText(error_frame, 'POSE DETECTION ERROR', (10, 50), \n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                        out.write(error_frame)\n",
    "                \n",
    "                frame_count += 1\n",
    "                \n",
    "                # Progress indicator\n",
    "                if frame_count % 30 == 0:\n",
    "                    progress = (frame_count / total_frames) * 100\n",
    "                    print(f\"Progress: {progress:.1f}% ({frame_count}/{total_frames} frames)\")\n",
    "            \n",
    "        finally:\n",
    "            cap.release()\n",
    "            if save_annotated_video:\n",
    "                out.release()\n",
    "        \n",
    "        # Detect reach cycles\n",
    "        reach_cycles = self.detect_reach_cycles(self.angles, self.confidences)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        proper_reaches = sum(1 for cycle in reach_cycles if cycle.is_proper_reach)\n",
    "        max_angle_overall = max(self.angles) if self.angles else 0\n",
    "        average_angle = np.mean(self.angles) if self.angles else 0\n",
    "        \n",
    "        # Create results object\n",
    "        results = AnalysisResults(\n",
    "            total_frames=total_frames,\n",
    "            valid_frames=valid_frames,\n",
    "            total_reach_attempts=len(reach_cycles),\n",
    "            proper_reaches=proper_reaches,\n",
    "            max_angle_overall=max_angle_overall,\n",
    "            average_angle=average_angle,\n",
    "            reach_cycles=reach_cycles,\n",
    "            angle_data=self.angles,\n",
    "            confidence_data=self.confidences,\n",
    "            error_frames=self.error_frames\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nAnalysis complete!\")\n",
    "        if save_annotated_video:\n",
    "            print(f\"Annotated video saved: {output_path}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"SitReachAnalyzer class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_analysis_results(results: AnalysisResults, save_plots: bool = True):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations of the analysis results\n",
    "    \n",
    "    Args:\n",
    "        results: AnalysisResults object\n",
    "        save_plots: Whether to save plots to files\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Sit and Reach Test Analysis Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Angle over time\n",
    "    axes[0, 0].plot(results.angle_data, linewidth=2, color='blue', alpha=0.7)\n",
    "    axes[0, 0].axhline(y=results.average_angle, color='green', linestyle='--', \n",
    "                      label=f'Average: {results.average_angle:.1f}°')\n",
    "    axes[0, 0].axhline(y=45, color='red', linestyle='--', \n",
    "                      label='Proper Reach Threshold: 45°')\n",
    "    axes[0, 0].set_title('Torso Angle Over Time')\n",
    "    axes[0, 0].set_xlabel('Frame Number')\n",
   ]
}
